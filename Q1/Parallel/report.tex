\documentclass{article}
\usepackage{ amssymb }
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{tikz}
\usepackage{float}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{hyperref}
\usepackage{listings}
\usepackage[super]{nth}
\usepackage[margin=0.8in]{geometry}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\usetikzlibrary{patterns}

\title{Lab Report Parallel Computing TW3740TU}
\author{Malte Wegener (4672194)}
\date{\today}

\begin{document}

\maketitle

\section{OpenMP}
\subsection{1}
\subsubsection{a)}
When executing the code, I did not expect the threads to print the message in a specific order. When executing the code multiple times this was confirmed

\subsubsection{b)}
\begin{lstlisting}
int tid, num_threads;

#pragma omp parallel private(tid, num_threads)
{
	tid=omp_get_thread_num();
	num_threads = omp_get_num_threads();
	printf("Hello from thread No %d from %d total threads.\n",tid, num_threads);
}

\end{lstlisting}

\subsection{2}
From running getenviroinfo.c, it can be seen that my PC has 4 cores, with 1 thread per core each, this fits with the lscpu command, which also shows that a Intel(R) Core(TM) i5-3570 is used.


\subsection{2}
\subsubsection{a)}
\begin{lstlisting}
fTimeStart = omp_get_wtime();
#pragma omp parallel for private(j)
for (i=0; i < SIZE; i++)
	for (j=0; j < SIZE; j++)
		c[i] = c[i] + A[j][i] * b[i];

fTimeEnd = omp_get_wtime();
\end{lstlisting}
With this code, the execution times are\\
\begin{center}
\begin{tabular}{c|c}
	Threads&Time [ms] \\ \hline
	1&8.2 \\ \hline
	2&4.6 \\ \hline
	4&3.1 \\ \hline
	8&4.1 \\ \hline
\end{tabular}
\end{center}
\subsubsection{b)}
As C arrays are row major, the code will be faster if it is changed to 
\begin{lstlisting}
fTimeStart = omp_get_wtime();
#pragma omp parallel for private(i)
for (j=0; j < SIZE; j++)
	for (i=0; i < SIZE; i++)
		c[i] = c[i] + A[j][i] * b[i];

fTimeEnd = omp_get_wtime();
\end{lstlisting}
\begin{center}
\begin{tabular}{c|c}
	Threads&Time [ms] \\ \hline
	1&3.2 \\ \hline
	2&3.0 \\ \hline
	4&1.9 \\ \hline
	8&3.3 \\ \hline
\end{tabular}
\end{center}

\subsection{4}
\subsubsection{a)}
The code is parallelized as
\begin{lstlisting}
#pragma omp parallel for reduction(+:fSum) private(fX)
for (i = 0; i < n; i += 1)
{
	fX = fH * ((double)i + 0.5);
	fSum += f(fX);
}
\end{lstlisting}
With this Speedup ($\frac{T_1}{T_p}$) and Efficiency ($\frac{S}{P}$), where P is the number of physical Processors.
\begin{center}
	\begin{tabular}{c|c|c|c}
		\# Threads&Runtime [s]&Speedup&Efficiency \\ \hline
		1&1.26&1&1\\ \hline
		2&0.63&2&1\\ \hline
		3&0.46&2.74&0.91\\ \hline
		4&0.48&2.63&0.66\\ \hline
		6&0.44&2.86&0.72\\ \hline
		8&0.47&2.68&0.67\\ \hline
		12&0.46&2.74&0.69
	\end{tabular}
\end{center}
\subsection{5}
The problem is a race condition for the variable j in the second for loop, this is fixed by making j private for each thread.
\begin{lstlisting}
#pragma omp parallel shared(nthreads) private(i,j,tid,a)
\end{lstlisting}

\subsection{6}
\subsubsection{a)}
The matrix multiplication is parallelized as
\begin{lstlisting}
 #pragma omp parallel for private(j,k) schedule(static, chunk)
for (i=0; i<NRA; i++)    
{
	for(j=0; j<NCB; j++)       
	for (k=0; k<NCA; k++)
	c[i][j] += a[i][k] * b[k][j];
}
\end{lstlisting}

\begin{center}
	\begin{tabular}{c|c}
		\# Threads & Runtime [s] \\ \hline
		1&0.422783\\ \hline
		2&0.198780\\ \hline
		4&		0.153811\\ \hline
		8&		0.138527
	\end{tabular}
\end{center}

\subsubsection{b)}
When increasing the chunk size, the execution time does not decrease nor increase significantly. When however decreasing the chunksize, execution on 8 and 4 threads is slightly faster.

\subsubsection{c)}
The speedup of the program plateaus after 4 threads, which is expected, as there are only 4 logical cores. Furthermore efficiency decreases, as there is overheap for the coordination of the threads.


\section{MPI}
\subsection{1}
\subsubsection{c)}
If there are less than 2 Processors, MPI can't send the message anywhere, so we abort execution of the program.
For more than 2 Processors, the array is send to every other node from node 0, and every node then sends back an array to node 0.
\subsubsection{e)}
When measuring the execution time for different message lengths, an approximation of the communication time can be derived.
\begin{equation}
	t_{comm} = \alpha + m * \beta
\end{equation}
By linear regression, $\alpha = 1.1 [ms]$ and $\beta = 4.328*10^{-10} [s/byte]$, or equivalently a bandwidth of $2.3 GB/s$. This si very high, but expected, as it is run on a local virtual cluster.
Runnig the test on the HPC Cluster, $\alpha = 2.2 [ms]$ and $\beta = 7.258*10^{-10} [s/byte]$ or equivalently a bandwidth of $1.3 GB/s$.

\subsection{2}
\subsubsection{a)}
No speedup is observed as every node is calculating every point, and the results of other nodes are not used.
\end{document}